"""
Clean and improved main application module with proper error handling
"""
import argparse
import asyncio
import os
import sys
import time
import traceback
import platform
from pathlib import Path
from typing import Optional, List, Dict, Any
import json

# Import core modules
from .core.config import config as app_config
from .core.data_parser import DataParser, ParserMetrics
from .core.checkpoint_manager import CheckpointManager, create_checkpoint_manager
from .processing.image_processor import ImageProcessorWithEmbedding, process_images_batch
from .utils.helpers import (
    ensure_directories,
    print_banner,
    select_file,
    select_formats,
    check_dependencies,
    check_system_resources,
    format_file_size,
    format_number,
    get_system_info
)
from .utils.logger import get_global_logger, setup_logging, log_system_info

# Setup logging
logger = get_global_logger()


class FaceRecognitionProcessor:
    """Main processor class with improved error handling and performance"""
    
    def __init__(self, output_dir: str, selected_formats: List[str]):
        self.output_dir = output_dir
        self.selected_formats = selected_formats
        self.checkpoint_manager = create_checkpoint_manager(output_dir)
        self.parser = DataParser()
        self.metrics = ParserMetrics()
        self.processing_stats = {
            'total_lines': 0,
            'processed_lines': 0,
            'valid_images': 0,
            'failed_images': 0,
            'json_errors': 0,
            'cached_images': 0,
            'network_errors': 0,
            'timeout_errors': 0,
            'duplicate_records': 0,
            'last_position': 0,
            'batch_size': app_config.initial_batch_size,
            'records_processed': [],
            'unique_users': set(),
            'unique_devices': set(),
            'unique_companies': set(),
            'unique_ips': set(),
            'start_time': time.time(),
            'last_update_time': time.time()
        }
        
        # Processing state
        self.is_processing = False
        self.resume_mode = False
        self.interrupted = False
    
    async def process_file(self, file_path: str):
        """Process the entire file with checkpoint support"""
        logger.info(f"Starting processing of {file_path}")
        
        # Load checkpoint if exists
        checkpoint = self.checkpoint_manager.load_checkpoint()
        if checkpoint:
            is_valid, error_msg = self.checkpoint_manager.validate_checkpoint(file_path)
            if is_valid:
                logger.info(f"Resuming from checkpoint: {checkpoint.processed_lines:,} records")
                self.resume_mode = True
                self._restore_from_checkpoint(checkpoint)
            else:
                logger.warning(f"Invalid checkpoint: {error_msg}")
        
        # Prepare output directories
        app_config.setup_directories(self.output_dir)
        
        # Get total file size
        total_size = os.path.getsize(file_path)
        logger.info(f"File size: {format_file_size(total_size)}")
        
        # Initialize image processor
        async with ImageProcessorWithEmbedding(self.output_dir) as image_processor:
            await self._process_file_with_image_processor(file_path, image_processor)
        
        # Generate reports
        await self._generate_reports()
        
        # Clean up
        self.checkpoint_manager.clear_checkpoint()
    
    def _restore_from_checkpoint(self, checkpoint):
        """Restore processing state from checkpoint"""
        self.processing_stats.update({
            'processed_lines': checkpoint.processed_lines,
            'valid_images': checkpoint.valid_images,
            'failed_images': checkpoint.failed_images,
            'json_errors': checkpoint.json_errors,
            'cached_images': checkpoint.cached_images,
            'network_errors': checkpoint.network_errors,
            'timeout_errors': checkpoint.timeout_errors,
            'duplicate_records': checkpoint.duplicate_records,
            'last_position': checkpoint.last_position,
            'batch_size': checkpoint.batch_size,
            'records_processed': checkpoint.records_processed,
            'unique_users': set(checkpoint.unique_users),
            'unique_devices': set(checkpoint.unique_devices),
            'unique_companies': set(checkpoint.unique_companies),
            'unique_ips': set(checkpoint.unique_ips),
        })
    
    async def _process_file_with_image_processor(self, file_path: str, image_processor: ImageProcessorWithEmbedding):
        """Process file with image processor"""
        # Get total lines
        self.processing_stats['total_lines'] = self._count_lines(file_path)
        logger.info(f"Total records to process: {format_number(self.processing_stats['total_lines'])}")
        
        # Open file
        with open(file_path, 'r', encoding='utf-8') as f:
            # If resuming, seek to checkpoint position
            if self.resume_mode and self.processing_stats['last_position'] > 0:
                f.seek(self.processing_stats['last_position'])
            
            batch = []
            batch_num = 0
            
            for line_num, line in enumerate(f, 1):
                if self.interrupted:
                    logger.info("Processing interrupted by user")
                    break
                
                # Skip already processed lines in resume mode
                if self.resume_mode and line_num <= self.processing_stats['processed_lines']:
                    continue
                
                line = line.strip()
                if not line:
                    continue
                
                batch.append(line)
                
                # Process batch when full
                if len(batch) >= self.processing_stats['batch_size']:
                    await self._process_batch(batch, image_processor)
                    batch = []
                    batch_num += 1
                    
                    # Update checkpoint
                    self._update_checkpoint(file_path)
                    
                    # Update progress
                    self._update_progress()
            
            # Process remaining batch
            if batch:
                await self._process_batch(batch, image_processor)
                self._update_checkpoint(file_path)
                self._update_progress()
    
    def _count_lines(self, file_path: str) -> int:
        """Count total lines in file"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return sum(1 for _ in f if _.strip())
        except Exception as e:
            logger.error(f"Error counting lines: {e}")
            return 0
    
    async def _process_batch(self, batch: List[str], image_processor: ImageProcessorWithEmbedding):
        """Process a batch of records"""
        # Parse records
        parsed_records = self.parser.parse_batch(batch, self.metrics)
        
        # Extract image URLs
        image_urls = []
        for record in parsed_records:
            if record.get('image_url'):
                image_urls.append(record['image_url'])
        
        # Process images if any
        if image_urls:
            try:
                image_results = await process_images_batch(image_processor, image_urls, self.metrics)
                
                # Update stats
                for result in image_results:
                    if result.filepath and result.base64_str:
                        self.processing_stats['valid_images'] += 1
                    else:
                        self.processing_stats['failed_images'] += 1
            except Exception as e:
                logger.error(f"Error processing images: {e}")
                self.processing_stats['network_errors'] += len(image_urls)
        
        # Update other stats
        self.processing_stats['processed_lines'] += len(batch)
        self.processing_stats['json_errors'] += len(batch) - len(parsed_records)
        
        # Update unique collections
        for record in parsed_records:
            if record.get('user_name'):
                self.processing_stats['unique_users'].add(record['user_name'])
            if record.get('device_id'):
                self.processing_stats['unique_devices'].add(record['device_id'])
            if record.get('company_id'):
                self.processing_stats['unique_companies'].add(record['company_id'])
            if record.get('ip_address'):
                self.processing_stats['unique_ips'].add(record['ip_address'])
    
    def _update_checkpoint(self, file_path: str):
        """Update checkpoint"""
        if not self.checkpoint_manager.should_save_checkpoint(
            self.processing_stats['processed_lines'] - 
            (self.checkpoint_manager.state.processed_lines if self.checkpoint_manager.state else 0)
        ):
            return
        
        success = self.checkpoint_manager.save_checkpoint(
            file_name=os.path.basename(file_path),
            total_lines=self.processing_stats['total_lines'],
            processed_lines=self.processing_stats['processed_lines'],
            valid_images=self.processing_stats['valid_images'],
            failed_images=self.processing_stats['failed_images'],
            json_errors=self.processing_stats['json_errors'],
            cached_images=self.processing_stats['cached_images'],
            network_errors=self.processing_stats['network_errors'],
            timeout_errors=self.processing_stats['timeout_errors'],
            duplicate_records=self.processing_stats['duplicate_records'],
            last_position=0,  # We're not tracking file position in this simplified version
            batch_size=self.processing_stats['batch_size'],
            records_processed=self.processing_stats['records_processed'],
            unique_users=list(self.processing_stats['unique_users']),
            unique_devices=list(self.processing_stats['unique_devices']),
            unique_companies=list(self.processing_stats['unique_companies']),
            unique_ips=list(self.processing_stats['unique_ips'])
        )
        
        if success:
            logger.debug(f"Checkpoint saved at {self.processing_stats['processed_lines']:,} records")
    
    def _update_progress(self):
        """Update progress information"""
        elapsed_time = time.time() - self.processing_stats['start_time']
        processed = self.processing_stats['processed_lines']
        total = self.processing_stats['total_lines']
        
        if total > 0:
            progress_percent = (processed / total) * 100
            records_per_sec = processed / elapsed_time if elapsed_time > 0 else 0
            remaining = total - processed
            eta_seconds = remaining / records_per_sec if records_per_sec > 0 else 0
            
            # Update every 5 seconds or every 1000 records
            current_time = time.time()
            if (current_time - self.processing_stats['last_update_time'] >= 5 or 
                processed % 1000 == 0):
                
                print(f"\rüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress_percent:.1f}% ({format_number(processed):>8s}/{format_number(total):>8s}) "
                      f"‚ö° {records_per_sec:.1f} rec/s "
                      f"‚è±Ô∏è ETA: {self._format_time(eta_seconds):>8s}", end='', flush=True)
                
                self.processing_stats['last_update_time'] = current_time
    
    def _format_time(self, seconds: float) -> str:
        """Format time in human readable format"""
        if seconds < 0:
            return "N/A"
        elif seconds < 60:
            return f"{seconds:.0f}s"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.1f}m"
        else:
            hours = seconds / 3600
            return f"{hours:.1f}h"
    
    async def _generate_reports(self):
        """Generate output reports"""
        logger.info("Generating reports...")
        
        # Create summary report
        summary = {
            'processing_summary': {
                'total_records': self.processing_stats['processed_lines'],
                'valid_images': self.processing_stats['valid_images'],
                'failed_images': self.processing_stats['failed_images'],
                'json_errors': self.processing_stats['json_errors'],
                'unique_users': len(self.processing_stats['unique_users']),
                'unique_devices': len(self.processing_stats['unique_devices']),
                'unique_companies': len(self.processing_stats['unique_companies']),
                'processing_time_seconds': time.time() - self.processing_stats['start_time'],
                'records_per_second': (
                    self.processing_stats['processed_lines'] / 
                    (time.time() - self.processing_stats['start_time'])
                    if time.time() - self.processing_stats['start_time'] > 0 else 0
                )
            },
            'selected_formats': self.selected_formats,
            'config': app_config.get_optimal_settings()
        }
        
        # Save summary
        summary_file = os.path.join(self.output_dir, app_config.summary_report)
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Summary report saved to: {summary_file}")
        
        # Generate format-specific reports based on selection
        for fmt in self.selected_formats:
            if fmt == "HTML":
                await self._generate_html_report()
            elif fmt == "PDF":
                await self._generate_pdf_report()
            elif fmt == "Excel":
                await self._generate_excel_report()
            elif fmt == "JSON":
                await self._generate_json_report()
    
    async def _generate_html_report(self):
        """Generate HTML report"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Face Recognition Report</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .thumbnail {{ width: 120px; height: 120px; object-fit: cover; }}
        .stats {{ background-color: #f9f9f9; padding: 15px; margin: 10px 0; border-radius: 5px; }}
    </style>
</head>
<body>
    <h1>Face Recognition Report</h1>
    
    <div class="stats">
        <h2>Processing Statistics</h2>
        <p>Total Records: {format_number(self.processing_stats['processed_lines'])}</p>
        <p>Valid Images: {format_number(self.processing_stats['valid_images'])}</p>
        <p>Failed Images: {format_number(self.processing_stats['failed_images'])}</p>
        <p>Unique Users: {format_number(len(self.processing_stats['unique_users']))}</p>
        <p>Unique Devices: {format_number(len(self.processing_stats['unique_devices']))}</p>
    </div>
    
    <h2>Sample Data</h2>
    <table>
        <thead>
            <tr>
                <th>Timestamp</th>
                <th>Device ID</th>
                <th>User Name</th>
                <th>Gender</th>
                <th>Age</th>
                <th>Score</th>
                <th>Image</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>2024-01-01 10:00:00</td>
                <td>CAM001</td>
                <td>–ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤</td>
                <td>–ú—É–∂—Å–∫–æ–π</td>
                <td>30</td>
                <td>95.5%</td>
                <td><img src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAABAAEDASIAAhEBAxEB/8QAFQABAQAAAAAAAAAAAAAAAAAAAAv/xAAUEAEAAAAAAAAAAAAAAAAAAAAA/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAX/xAAUEQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIRAxEAPwAfgA==" class="thumbnail" alt="Sample Image"></td>
            </tr>
        </tbody>
    </table>
</body>
</html>
"""
        
        html_file = os.path.join(self.output_dir, app_config.reports_folder, app_config.html_report)
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"HTML report saved to: {html_file}")
    
    async def _generate_pdf_report(self):
        """Generate PDF report placeholder"""
        logger.info("PDF report generation skipped (requires reportlab)")
    
    async def _generate_excel_report(self):
        """Generate Excel report placeholder"""
        logger.info("Excel report generation skipped (requires openpyxl)")
    
    async def _generate_json_report(self):
        """Generate JSON report with statistics"""
        json_data = {
            'metadata': {
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_records': self.processing_stats['processed_lines'],
                'valid_images': self.processing_stats['valid_images'],
                'failed_images': self.processing_stats['failed_images']
            },
            'statistics': {
                'unique_users': len(self.processing_stats['unique_users']),
                'unique_devices': len(self.processing_stats['unique_devices']),
                'unique_companies': len(self.processing_stats['unique_companies']),
                'processing_time_seconds': time.time() - self.processing_stats['start_time']
            }
        }
        
        json_file = os.path.join(self.output_dir, app_config.reports_folder, 'statistics.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"JSON report saved to: {json_file}")


async def run_processing(input_file: str, selected_formats: List[str], output_dir: str):
    """Run the main processing workflow"""
    logger.info("Starting face recognition processing...")
    
    # Create processor
    processor = FaceRecognitionProcessor(output_dir, selected_formats)
    
    try:
        # Process file
        await processor.process_file(input_file)
        
        # Print final statistics
        print("\n" + "="*80)
        print("üìä –û–ö–û–ù–ß–ê–¢–ï–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("="*80)
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {format_number(processor.processing_stats['processed_lines']):>15s}")
        print(f"–£—Å–ø–µ—à–Ω—ã—Ö —Ñ–æ—Ç–æ:     {format_number(processor.processing_stats['valid_images']):>15s}")
        print(f"–û—à–∏–±–æ–∫ —Ñ–æ—Ç–æ:       {format_number(processor.processing_stats['failed_images']):>15s}")
        print(f"–û—à–∏–±–æ–∫ JSON:       {format_number(processor.processing_stats['json_errors']):>15s}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —é–∑–µ—Ä–æ–≤: {format_number(len(processor.processing_stats['unique_users'])):>15s}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—Å—Ç—Ä.:  {format_number(len(processor.processing_stats['unique_devices'])):>15s}")
        print("="*80)
        
        logger.info("Processing completed successfully")
        return True
        
    except KeyboardInterrupt:
        logger.info("Processing interrupted by user")
        print("\n‚ö†Ô∏è  –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return False
    except Exception as e:
        logger.error(f"Error during processing: {e}")
        traceback.print_exc()
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        return False


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Face Recognition Analytics Suite",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python main.py --file data.json --format HTML
  python main.py --file data.json --format HTML,Excel --output results_2024
  python main.py --resume
        """
    )
    
    parser.add_argument(
        '--file',
        type=str,
        help='–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É JSON/JSONL'
    )
    
    parser.add_argument(
        '--format',
        type=str,
        help='–§–æ—Ä–º–∞—Ç—ã –æ—Ç—á–µ—Ç–æ–≤ (HTML,PDF,Excel,JSON) —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='–ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)'
    )
    
    parser.add_argument(
        '--resume',
        action='store_true',
        help='–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –ø—Ä–µ—Ä–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É'
    )
    
    parser.add_argument(
        '--no-interaction',
        action='store_true',
        help='–†–µ–∂–∏–º –±–µ–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤'
    )
    
    parser.add_argument(
        '--no-check',
        action='store_true',
        help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π'
    )
    
    return parser.parse_args()


def setup_asyncio_for_platform():
    """Setup asyncio for different platforms"""
    if platform.system() == "Windows":
        if sys.version_info >= (3, 8):
            import asyncio
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    else:
        # For Unix systems, try to use uvloop if available
        try:
            import uvloop
            uvloop.install()
        except ImportError:
            pass


async def main():
    """Main application entry point"""
    print_banner()
    
    # Parse arguments
    args = parse_arguments()
    
    # Setup asyncio
    setup_asyncio_for_platform()
    
    # Check Python version
    if sys.version_info < (3, 7):
        print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è Python 3.7 –∏–ª–∏ –≤—ã—à–µ")
        return False
    
    # Ensure directories exist
    ensure_directories()
    
    # Check system resources
    if not check_system_resources():
        print("‚ö†Ô∏è  –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        if not args.no_interaction:
            input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")
        return False
    
    # Get input file
    if args.file:
        input_file = args.file
    else:
        input_file = select_file()
        if not input_file:
            print("‚ùå –§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω")
            return False
    
    # Validate file
    is_valid, error_msg = validate_file_path(input_file)
    if not is_valid:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞: {error_msg}")
        return False
    
    # Get selected formats
    if args.format:
        selected_formats = [fmt.strip().upper() for fmt in args.format.split(',')]
    else:
        selected_formats = select_formats()
        if not selected_formats:
            print("‚ùå –§–æ—Ä–º–∞—Ç—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã")
            return False
    
    # Check dependencies if not skipped
    if not args.no_check:
        if not check_dependencies(selected_formats):
            print("‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞")
            return False
    
    # Get output directory
    if args.output:
        output_dir = args.output
    else:
        output_dir = app_config.get_output_subdir()
    
    print(f"\nüìÅ –ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {output_dir}")
    
    # Confirm before starting
    if not args.no_interaction:
        confirm = input("\nüëâ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É? (y/N): ").strip().lower()
        if confirm != 'y':
            print("‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
            return False
    
    # Run processing
    success = await run_processing(input_file, selected_formats, output_dir)
    
    if success:
        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
        
        # Show results location
        reports_dir = os.path.join(output_dir, app_config.reports_folder)
        if os.path.exists(reports_dir):
            print(f"üìä –û—Ç—á–µ—Ç—ã: {reports_dir}")
        
        images_dir = os.path.join(output_dir, app_config.image_folder)
        if os.path.exists(images_dir):
            print(f"üñºÔ∏è  –§–æ—Ç–æ: {images_dir}")
    else:
        print(f"\n‚ö†Ô∏è  –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å —á–∞—Å—Ç–∏—á–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
    
    return success


def validate_file_path(file_path: str) -> tuple[bool, str]:
    """Validate file path"""
    if not os.path.exists(file_path):
        return False, "–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"
    
    if not os.path.isfile(file_path):
        return False, "–£–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–∞–π–ª–æ–º"
    
    if os.path.getsize(file_path) == 0:
        return False, "–§–∞–π–ª –ø—É—Å—Ç"
    
    # Check file extension
    valid_extensions = ['.json', '.jsonl', '.txt']
    file_ext = os.path.splitext(file_path)[1].lower()
    
    if file_ext not in valid_extensions:
        return False, f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_ext}"
    
    return True, "–§–∞–π–ª –≤–∞–ª–∏–¥–µ–Ω"


if __name__ == "__main__":
    try:
        # Run main function
        success = asyncio.run(main())
        
        if not success:
            sys.exit(1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        traceback.print_exc()
        sys.exit(1)
    finally:
        if 'main' in sys.argv or '--help' not in sys.argv:
            input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞...")