"""
–ü–∞—Ä—Å–µ—Ä JSON –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
"""

import json
import re
import hashlib
import time
import functools
import logging
from typing import Optional, Dict, Tuple, List, Any, Callable, Union
from collections import defaultdict, OrderedDict
from dataclasses import dataclass, field
from contextlib import contextmanager
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# –ü–æ–ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å ujson –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, –∏–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π json
try:
    import ujson
    JSON = ujson
    JSON_DECODE_ERROR = ujson.JSONDecodeError
except ImportError:
    JSON = json
    JSON_DECODE_ERROR = json.JSONDecodeError

logger = logging.getLogger(__name__)


@dataclass
class ParserConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    max_cache_size: int = 10000
    cache_ttl_seconds: int = 3600
    enable_cache: bool = True
    batch_size: int = 1000
    max_retries: int = 2
    validation_enabled: bool = True
    strict_mode: bool = False


@dataclass
class ParserMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞"""
    total_parsed: int = 0
    total_errors: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    parse_time_total: float = 0.0
    last_reset_time: float = field(default_factory=time.time)
    
    @property
    def cache_hit_rate(self) -> float:
        total_accesses = self.cache_hits + self.cache_misses
        return (self.cache_hits / total_accesses * 100) if total_accesses > 0 else 0.0
    
    @property
    def avg_parse_time_ms(self) -> float:
        return (self.parse_time_total / self.total_parsed * 1000) if self.total_parsed > 0 else 0.0
    
    @property
    def error_rate(self) -> float:
        total_processed = self.total_parsed + self.total_errors
        return (self.total_errors / total_processed * 100) if total_processed > 0 else 0.0
    
    def reset(self):
        """–°–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
        self.total_parsed = 0
        self.total_errors = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.parse_time_total = 0.0
        self.last_reset_time = time.time()
    
    def to_dict(self) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'total_parsed': self.total_parsed,
            'total_errors': self.total_errors,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'cache_hit_rate': self.cache_hit_rate,
            'avg_parse_time_ms': self.avg_parse_time_ms,
            'error_rate': self.error_rate,
            'uptime_seconds': time.time() - self.last_reset_time
        }


class CacheManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ –∂–∏–∑–Ω–∏"""
    
    def __init__(self, max_size: int = 10000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self._cache = OrderedDict()
        self._timestamps = {}
        self._size_bytes = 0
        self._hits = 0
        self._misses = 0
        
    def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞"""
        if key not in self._cache:
            self._misses += 1
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
        timestamp = self._timestamps.get(key)
        if timestamp and (time.time() - timestamp) > self.ttl_seconds:
            self._remove(key)
            self._misses += 1
            return None
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∫–æ–Ω–µ—Ü (—Å–¥–µ–ª–∞–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–º)
        value = self._cache.pop(key)
        self._cache[key] = value
        
        self._hits += 1
        return value
    
    def set(self, key: str, value: Any) -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫—ç—à"""
        # –ï—Å–ª–∏ –∫–ª—é—á —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        if key in self._cache:
            self._remove(key)
        
        # –û—á–∏—â–∞–µ–º –º–µ—Å—Ç–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        while len(self._cache) >= self.max_size:
            self._remove_oldest()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        self._cache[key] = value
        self._timestamps[key] = time.time()
        self._size_bytes += self._estimate_size(key, value)
    
    def _remove(self, key: str) -> None:
        """–£–¥–∞–ª–∏—Ç—å –∫–ª—é—á –∏–∑ –∫—ç—à–∞"""
        if key in self._cache:
            value = self._cache.pop(key)
            self._timestamps.pop(key, None)
            self._size_bytes -= self._estimate_size(key, value)
    
    def _remove_oldest(self) -> None:
        """–£–¥–∞–ª–∏—Ç—å —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç"""
        if self._cache:
            key, value = self._cache.popitem(last=False)
            self._timestamps.pop(key, None)
            self._size_bytes -= self._estimate_size(key, value)
    
    def _estimate_size(self, key: str, value: Any) -> int:
        """–û—Ü–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö"""
        size = len(key.encode('utf-8'))  # UTF-8 –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        
        if isinstance(value, dict):
            for k, v in value.items():
                size += len(str(k).encode('utf-8'))
                size += len(str(v).encode('utf-8'))
        elif isinstance(value, (list, tuple)):
            for item in value:
                size += len(str(item).encode('utf-8'))
        else:
            size += len(str(value).encode('utf-8'))
        
        return size
    
    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à"""
        self._cache.clear()
        self._timestamps.clear()
        self._size_bytes = 0
        self._hits = 0
        self._misses = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        total_accesses = self._hits + self._misses
        hit_rate = (self._hits / total_accesses * 100) if total_accesses > 0 else 0.0
        return {
            'size': len(self._cache),
            'max_size': self.max_size,
            'size_bytes': self._size_bytes,
            'hits': self._hits,
            'misses': self._misses,
            'hit_rate': f"{hit_rate:.1f}%",
            'ttl_seconds': self.ttl_seconds
        }


class FieldExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π –∏–∑ JSON —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä"""
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    _FIELD_PATTERNS = {
        'timestamp': re.compile(r'"timestamp"\s*:\s*(?:\{"\$date"\s*:\s*"([^"]+)"\}|"([^"]+)")'),
        'device_id': re.compile(r'"device_id"\s*:\s*"([^"]+)"'),
        'user_name': re.compile(r'"user_name"\s*:\s*"([^"]+)"'),
        'eva_sex': re.compile(r'"eva_sex"\s*:\s*(?:"([^"]+)"|(\d+|null|true|false))'),
        'sex': re.compile(r'"sex"\s*:\s*(?:"([^"]+)"|(\d+|null|true|false))'),
        'comp_score': re.compile(r'"comp_score"\s*:\s*(?:"([^"]+)"|([\d\.]+|null|true|false))'),
        'eva_age': re.compile(r'"eva_age"\s*:\s*(?:"([^"]+)"|([\d\.]+|null|true|false))'),
        'image': re.compile(r'"image"\s*:\s*"([^"]+)"'),
        'face_id': re.compile(r'"face_id"\s*:\s*(?:"([^"]+)"|([\d\.]+|null|true|false))'),
        'company_id': re.compile(r'"company_id"\s*:\s*(?:"([^"]+)"|([\d\.]+|null|true|false))'),
        'event_type': re.compile(r'"event_type"\s*:\s*(?:"([^"]+)"|(\d+|null|true|false))'),
        'user_list': re.compile(r'"user_list"\s*:\s*(?:"([^"]+)"|(\d+|null|true|false))'),
        'ip_address': re.compile(r'"(?:IP|device_ip)"\s*:\s*"([^"]+)"'),
    }
    
    @classmethod
    def extract_fields_fast(cls, line: str) -> Optional[Dict[str, Any]]:
        """
        –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
        """
        if not line or len(line) < 10 or not line.strip().startswith('{'):
            return None
        
        result = {}
        
        try:
            for field_name, pattern in cls._FIELD_PATTERNS.items():
                match = pattern.search(line)
                if match:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –Ω–µ–ø—É—Å—Ç—É—é –≥—Ä—É–ø–ø—É
                    groups = match.groups()
                    value = next((g for g in groups if g is not None), None)
                    if value:
                        result[field_name] = value
            
            return result if result else None
            
        except Exception:
            return None
    
    @classmethod
    def is_valid_json_line(cls, line: str) -> bool:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ JSON —Å—Ç—Ä–æ–∫–∏"""
        line = line.strip()
        
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–µ—Ä–≤—ã–º –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–º —Å–∏–º–≤–æ–ª–∞–º
        if len(line) < 2:
            return False
        
        if not (line.startswith('{') and line.endswith('}')):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
        if '"timestamp"' not in line and '"device_id"' not in line:
            return False
        
        return True


class ValueTransformer:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    _cache_size = 10000
    
    @staticmethod
    @functools.lru_cache(maxsize=_cache_size)
    def transform_timestamp(value: Optional[str]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ timestamp"""
        if not value or value.lower() in ('null', 'none', 'nan', ''):
            return "–ù/–î"
        
        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ MongoDB
            if '$date' in value:
                try:
                    # –£–±–∏—Ä–∞–µ–º —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
                    import json as json_module
                    # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ JSON
                    if value.strip().startswith('{'):
                        data = json_module.loads(value)
                        value = data.get('$date', '')
                    else:
                        # –ò—â–µ–º $date –≤ —Å—Ç—Ä–æ–∫–µ
                        import re
                        match = re.search(r'\$date["\']?\s*:\s*["\']?([^"\'\s}]+)', value)
                        if match:
                            value = match.group(1)
                except:
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    pass
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ISO —Ñ–æ—Ä–º–∞—Ç–∞
            if 'T' in value:
                # –£–±–∏—Ä–∞–µ–º Z –∏ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                if value.endswith('Z'):
                    value = value[:-1]
                # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
                parts = value.split('T')
                if len(parts) == 2:
                    date_part = parts[0]
                    time_part = parts[1].split('.')[0]  # –£–±–∏—Ä–∞–µ–º –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                    return f"{date_part} {time_part}"
            
            return value
            
        except Exception:
            return "–ù/–î"
    
    @staticmethod
    @functools.lru_cache(maxsize=_cache_size)
    def transform_gender(eva_sex: Optional[str], sex: Optional[str]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–∞"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–µ –∏ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        eva_sex_str = str(eva_sex).lower().strip() if eva_sex else ''
        sex_str = str(sex).lower().strip() if sex else ''
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        gender_map = {
            # eva_sex –∑–Ω–∞—á–µ–Ω–∏—è
            'female': "–ñ–µ–Ω—Å–∫–∏–π",
            'f': "–ñ–µ–Ω—Å–∫–∏–π",
            '–∂–µ–Ω': "–ñ–µ–Ω—Å–∫–∏–π",
            '0': "–ñ–µ–Ω—Å–∫–∏–π",
            'male': "–ú—É–∂—Å–∫–æ–π",
            'm': "–ú—É–∂—Å–∫–æ–π",
            '–º—É–∂': "–ú—É–∂—Å–∫–æ–π",
            '1': "–ú—É–∂—Å–∫–æ–π",
            
            # sex –∑–Ω–∞—á–µ–Ω–∏—è
            '0': "–ñ–µ–Ω—Å–∫–∏–π",
            '1': "–ú—É–∂—Å–∫–æ–π",
            
            # –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            'null': "–ù/–î",
            'none': "–ù/–î",
            'nan': "–ù/–î",
            '': "–ù/–î",
        }
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º eva_sex
        if eva_sex_str and eva_sex_str in gender_map:
            return gender_map[eva_sex_str]
        
        # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º sex
        if sex_str and sex_str in gender_map:
            return gender_map[sex_str]
        
        return "–ù/–î"
    
    @staticmethod
    @functools.lru_cache(maxsize=_cache_size)
    def transform_score(value: Optional[str]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏"""
        if not value or value.lower() in ('null', 'none', 'nan', ''):
            return "–ù/–î"
        
        try:
            # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –∏ –ø—Ä–æ–±–µ–ª—ã
            clean_value = ''.join(c for c in str(value) if c.isdigit() or c == '.')
            if not clean_value:
                return "–ù/–î"
            
            score = float(clean_value)
            return f"{score:.1f}%"
            
        except Exception:
            return "–ù/–î"
    
    @staticmethod
    @functools.lru_cache(maxsize=_cache_size)
    def transform_age(value: Optional[str]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞"""
        if not value or value.lower() in ('null', 'none', 'nan', ''):
            return "–ù/–î"
        
        try:
            age = int(float(str(value)))
            return str(age)
        except Exception:
            return "–ù/–î"
    
    @staticmethod
    def safe_str(value: Any, default: str = '') -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É"""
        if value is None:
            return default
        
        try:
            result = str(value).strip()
            return result if result else default
        except Exception:
            return default


class DataParser:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä JSON –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞
    - –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    - –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–µ–π
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, config: Optional[ParserConfig] = None):
        self.config = config or ParserConfig()
        self.metrics = ParserMetrics()
        
        # –ö—ç—à–∏
        self.cache_manager = CacheManager(
            max_size=self.config.max_cache_size,
            ttl_seconds=self.config.cache_ttl_seconds
        ) if self.config.enable_cache else None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._batch_stats = defaultdict(int)
        self._error_stats = defaultdict(int)
        
        # –•—ç—à-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ç—Ä–æ–∫
        self._hash_func = hashlib.md5
        
        logger.info(f"DataParser –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: cache={self.config.enable_cache}, "
                   f"batch_size={self.config.batch_size}")
    
    def _generate_line_hash(self, line: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö—ç—à–∞ —Å—Ç—Ä–æ–∫–∏"""
        return self._hash_func(line.encode('utf-8')).hexdigest()[:16]
    
    def parse_record(self, line: str, metrics: Optional[Any] = None) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        
        Args:
            line: –°—Ç—Ä–æ–∫–∞ JSON –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            metrics: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        
        Returns:
            Dict —Å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        start_time = time.time()
        
        # –ë—ã—Å—Ç—Ä–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏
        if not FieldExtractor.is_valid_json_line(line):
            self.metrics.total_errors += 1
            self._error_stats['invalid_format'] += 1
            if metrics and hasattr(metrics, 'json_errors'):
                metrics.json_errors += 1
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        line_hash = None
        cached_result = None
        
        if self.config.enable_cache and self.cache_manager:
            line_hash = self._generate_line_hash(line)
            cached_result = self.cache_manager.get(line_hash)
            
            if cached_result:
                self.metrics.cache_hits += 1
                self.metrics.total_parsed += 1
                self.metrics.parse_time_total += time.time() - start_time
                
                if metrics and hasattr(metrics, 'total_records'):
                    metrics.total_records += 1
                
                return cached_result
        
        self.metrics.cache_misses += 1
        
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON
            data = JSON.loads(line.strip())
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            result = self._extract_fields(data)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if self.config.validation_enabled and not self._validate_record(result):
                self.metrics.total_errors += 1
                self._error_stats['validation_failed'] += 1
                if metrics and hasattr(metrics, 'json_errors'):
                    metrics.json_errors += 1
                return None
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            if self.config.enable_cache and self.cache_manager and line_hash:
                self.cache_manager.set(line_hash, result)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            self.metrics.total_parsed += 1
            parse_time = time.time() - start_time
            self.metrics.parse_time_total += parse_time
            
            if metrics and hasattr(metrics, 'total_records'):
                metrics.total_records += 1
            
            return result
            
        except JSON_DECODE_ERROR as e:
            self.metrics.total_errors += 1
            self._error_stats['json_decode'] += 1
            
            if metrics and hasattr(metrics, 'json_errors'):
                metrics.json_errors += 1
            
            logger.debug(f"JSON decode error: {e} for line: {line[:100]}...")
            return None
            
        except Exception as e:
            self.metrics.total_errors += 1
            self._error_stats['unexpected'] += 1
            
            if metrics and hasattr(metrics, 'json_errors'):
                metrics.json_errors += 1
            
            logger.error(f"Unexpected parsing error: {e}")
            
            # –í —Å—Ç—Ä–æ–≥–æ–º —Ä–µ–∂–∏–º–µ –ø–µ—Ä–µ–≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            if self.config.strict_mode and not isinstance(e, (MemoryError, KeyboardInterrupt)):
                raise
            
            return None
    
    def _extract_fields(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–µ–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ timestamp
        timestamp_raw = ""
        timestamp_obj = data.get('timestamp', {})
        if isinstance(timestamp_obj, dict):
            timestamp_raw = timestamp_obj.get('$date', '')
        elif isinstance(timestamp_obj, str):
            timestamp_raw = timestamp_obj
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π
        eva_sex = data.get('eva_sex', '')
        sex = data.get('sex', '')
        comp_score = data.get('comp_score', '')
        eva_age = data.get('eva_age', '')
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ IP –∞–¥—Ä–µ—Å–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö)
        ip_address = data.get('IP', data.get('device_ip', ''))
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ MongoDB _id
        mongo_id = ""
        mongo_id_obj = data.get('_id', {})
        if isinstance(mongo_id_obj, dict):
            mongo_id = mongo_id_obj.get('$oid', '')
        elif mongo_id_obj:
            mongo_id = str(mongo_id_obj)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
        timestamp = ValueTransformer.transform_timestamp(timestamp_raw)
        gender = ValueTransformer.transform_gender(eva_sex, sex)
        score = ValueTransformer.transform_score(comp_score)
        age = ValueTransformer.transform_age(eva_age)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        return {
            'timestamp': timestamp,
            'device_id': ValueTransformer.safe_str(data.get('device_id', ''), "–ù/–î"),
            'user_name': ValueTransformer.safe_str(data.get('user_name', ''), "–ù/–î"),
            'gender': gender,
            'age': age,
            'score': score,
            'face_id': ValueTransformer.safe_str(data.get('face_id', ''), "–ù/–î"),
            'company_id': ValueTransformer.safe_str(data.get('company_id', ''), "–ù/–î"),
            'image_url': ValueTransformer.safe_str(data.get('image', ''), ""),
            'event_type': ValueTransformer.safe_str(data.get('event_type', ''), ""),
            'user_list': ValueTransformer.safe_str(data.get('user_list', ''), ""),
            'ip_address': ValueTransformer.safe_str(ip_address, "–ù/–î"),
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            'user_id': ValueTransformer.safe_str(data.get('user_id', ''), ""),
            'frpic_name': ValueTransformer.safe_str(data.get('frpic_name', ''), ""),
            'request_type': ValueTransformer.safe_str(data.get('request_type', ''), ""),
            'group': ValueTransformer.safe_str(data.get('group', ''), ""),
            'mongo_id': mongo_id,
            'company_type': ValueTransformer.safe_str(data.get('company_type', ''), "")
        }
    
    def _validate_record(self, record: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_fields = ['timestamp', 'device_id', 'user_name']
        
        for field in required_fields:
            if not record.get(field) or record[field] == '–ù/–î':
                return False
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if 'image_url' in record and record['image_url']:
            if not record['image_url'].startswith(('http://', 'https://')):
                return False
        
        return True
    
    def parse_batch(self, lines: List[str], metrics: Optional[Any] = None) -> List[Dict[str, Any]]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø–∏—Å–µ–π
        
        Args:
            lines: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            metrics: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        start_time = time.time()
        batch_size = len(lines)
        results = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–æ–¥–±–∞—Ç—á–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞–º—è—Ç–∏
        batch_size_actual = min(batch_size, self.config.batch_size)
        
        for i in range(0, batch_size, batch_size_actual):
            sub_batch = lines[i:i + batch_size_actual]
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥–±–∞—Ç—á–∞
            for line in sub_batch:
                result = self.parse_record(line, metrics)
                if result:
                    results.append(result)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            if i % (batch_size_actual * 10) == 0 and self.cache_manager:
                # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏
                self._clean_expired_cache()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        parse_time = time.time() - start_time
        self._batch_stats['total_batches'] += 1
        self._batch_stats['total_records_in_batches'] += batch_size
        self._batch_stats['total_time_in_batches'] += parse_time
        
        logger.debug(f"–ü–∞–∫–µ—Ç –∏–∑ {batch_size} –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {parse_time:.3f} —Å–µ–∫, "
                    f"—É—Å–ø–µ—à–Ω–æ: {len(results)}")
        
        return results
    
    def _clean_expired_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ –∫—ç—à–µ"""
        if not self.cache_manager:
            return
        
        current_time = time.time()
        expired_keys = []
        
        # –ù–∞—Ö–æ–¥–∏–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∫–ª—é—á–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã)
        for key, timestamp in self.cache_manager._timestamps.items():
            if (current_time - timestamp) > self.cache_manager.ttl_seconds:
                expired_keys.append(key)
        
        # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏
        for key in expired_keys:
            if key in self.cache_manager._cache:
                self.cache_manager._cache.pop(key, None)
                self.cache_manager._timestamps.pop(key, None)
        
        if expired_keys:
            logger.debug(f"–û—á–∏—â–µ–Ω–æ {len(expired_keys)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –∫—ç—à–∞")
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à"""
        if self.cache_manager:
            self.cache_manager.clear()
            logger.info("–ö—ç—à –ø–∞—Ä—Å–µ—Ä–∞ –æ—á–∏—â–µ–Ω")
    
    def reset_metrics(self):
        """–°–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏"""
        self.metrics.reset()
        self._batch_stats.clear()
        self._error_stats.clear()
        logger.info("–ú–µ—Ç—Ä–∏–∫–∏ –ø–∞—Ä—Å–µ—Ä–∞ —Å–±—Ä–æ—à–µ–Ω—ã")
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–µ—Ä–∞"""
        stats = {
            'metrics': self.metrics.to_dict(),
            'cache': self.cache_manager.get_stats() if self.cache_manager else None,
            'batch_stats': dict(self._batch_stats),
            'error_stats': dict(self._error_stats),
            'config': {
                'max_cache_size': self.config.max_cache_size,
                'cache_ttl_seconds': self.config.cache_ttl_seconds,
                'enable_cache': self.config.enable_cache,
                'batch_size': self.config.batch_size,
                'validation_enabled': self.config.validation_enabled,
                'strict_mode': self.config.strict_mode
            }
        }
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –ø–∞—Ä—Å–∏–Ω–≥–∞
        uptime = time.time() - self.metrics.last_reset_time
        if uptime > 0:
            stats['parse_speed_records_per_second'] = self.metrics.total_parsed / uptime
            stats['parse_speed_records_per_hour'] = (self.metrics.total_parsed / uptime) * 3600
        
        return stats
    
    def get_performance_report(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        stats = self.get_statistics()
        
        report_lines = [
            "üìä –û–¢–ß–ï–¢ –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò –ü–ê–†–°–ï–†–ê",
            "=" * 50,
            f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {stats['metrics']['total_parsed']:,}",
            f"–û—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞: {stats['metrics']['total_errors']:,}",
            f"–ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫: {stats['metrics']['error_rate']:.1f}%",
            f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {stats['metrics']['avg_parse_time_ms']:.2f} –º—Å",
            "",
            "–ö—ç—à:",
        ]
        
        if stats['cache']:
            report_lines.extend([
                f"  –•–∏—Ç—ã: {stats['cache']['hits']:,}",
                f"  –ü—Ä–æ–º–∞—Ö–∏: {stats['cache']['misses']:,}",
                f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {stats['cache']['hit_rate']}",
                f"  –†–∞–∑–º–µ—Ä: {stats['cache']['size']:,}/{stats['cache']['max_size']:,}"
            ])
        else:
            report_lines.append("  –ö—ç—à –æ—Ç–∫–ª—é—á–µ–Ω")
        
        report_lines.append("")
        report_lines.append("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫:")
        
        for error_type, count in stats['error_stats'].items():
            report_lines.append(f"  {error_type}: {count:,}")
        
        if 'parse_speed_records_per_hour' in stats:
            report_lines.append(f"\n–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stats['parse_speed_records_per_hour']:,.0f} –∑–∞–ø–∏—Å–µ–π/—á–∞—Å")
        
        return "\n".join(report_lines)


class FastDataParser:
    """
    –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –æ–¥–Ω–æ—Ä–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π —Ñ–æ—Ä–º–∞—Ç–∞
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–µ—Å—Ç–Ω–∞ –∑–∞—Ä–∞–Ω–µ–µ –∏ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è
    """
    
    def __init__(self, field_config: Optional[Dict[str, Dict]] = None):
        self.field_config = field_config or {}
        self.field_extractors = self._build_extractors()
        self.metrics = ParserMetrics()
        
    def _build_extractors(self) -> Dict[str, Callable]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        extractors = {}
        
        for field_name, config in self.field_config.items():
            path = config.get('path', [field_name])
            default = config.get('default', '')
            transformer = config.get('transformer', lambda x: x)
            
            def make_extractor(field_path, field_default, field_transformer):
                def extractor(data: Dict) -> Any:
                    value = data
                    for key in field_path:
                        if isinstance(value, dict):
                            value = value.get(key, None)
                        else:
                            value = None
                            break
                    
                    if value is None or value == '':
                        return field_default
                    
                    return field_transformer(value)
                
                return extractor
            
            extractors[field_name] = make_extractor(path, default, transformer)
        
        return extractors
    
    def parse_record_fast(self, line: str) -> Optional[Dict[str, Any]]:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø–∏—Å–∏ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º"""
        try:
            data = JSON.loads(line.strip())
            self.metrics.total_parsed += 1
            
            result = {}
            for field_name, extractor in self.field_extractors.items():
                result[field_name] = extractor(data)
            
            return result
            
        except Exception as e:
            self.metrics.total_errors += 1
            logger.debug(f"Fast parsing error: {e}")
            return None
    
    def parse_batch_fast(self, lines: List[str]) -> List[Dict[str, Any]]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –±—ã—Å—Ç—Ä—ã–º –º–µ—Ç–æ–¥–æ–º"""
        results = []
        
        for line in lines:
            record = self.parse_record_fast(line)
            if record:
                results.append(record)
        
        return results


# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ñ–æ—Ä–º–∞—Ç–æ–≤
STANDARD_FORMAT_CONFIG = {
    'timestamp': {
        'path': ['timestamp', '$date'],
        'default': '–ù/–î',
        'transformer': ValueTransformer.transform_timestamp
    },
    'device_id': {
        'path': ['device_id'],
        'default': '–ù/–î',
        'transformer': ValueTransformer.safe_str
    },
    'user_name': {
        'path': ['user_name'],
        'default': '–ù/–î',
        'transformer': ValueTransformer.safe_str
    },
    'gender': {
        'path': ['eva_sex', 'sex'],
        'default': '–ù/–î',
        'transformer': lambda x: ValueTransformer.transform_gender(
            x[0] if isinstance(x, list) and len(x) > 0 else '',
            x[1] if isinstance(x, list) and len(x) > 1 else ''
        )
    },
    'age': {
        'path': ['eva_age'],
        'default': '–ù/–î',
        'transformer': ValueTransformer.transform_age
    },
    'score': {
        'path': ['comp_score'],
        'default': '–ù/–î',
        'transformer': ValueTransformer.transform_score
    }
}


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
_global_parser: Optional[DataParser] = None


def get_global_parser(config: Optional[ParserConfig] = None) -> DataParser:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞"""
    global _global_parser
    
    if _global_parser is None:
        _global_parser = DataParser(config)
    elif config is not None:
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–∞
        _global_parser.config = config
    
    return _global_parser


def create_standard_parser() -> DataParser:
    """–°–æ–∑–¥–∞—Ç—å –ø–∞—Ä—Å–µ—Ä —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    config = ParserConfig(
        max_cache_size=15000,
        cache_ttl_seconds=7200,
        enable_cache=True,
        batch_size=2000,
        validation_enabled=True,
        strict_mode=False
    )
    
    return DataParser(config)


def create_fast_parser(field_config: Optional[Dict] = None) -> FastDataParser:
    """–°–æ–∑–¥–∞—Ç—å –±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä —Å –∑–∞–¥–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø–æ–ª–µ–π"""
    config = field_config or STANDARD_FORMAT_CONFIG
    return FastDataParser(config)


# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
def parse_single_record(line: str, metrics: Optional[Any] = None) -> Optional[Dict[str, Any]]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    parser = get_global_parser()
    return parser.parse_record(line, metrics)


def parse_batch_records(lines: List[str], metrics: Optional[Any] = None) -> List[Dict[str, Any]]:
    """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø–∏—Å–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    parser = get_global_parser()
    return parser.parse_batch(lines, metrics)


def extract_key_fields_fast(line: str) -> Optional[Dict[str, str]]:
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    return FieldExtractor.extract_fields_fast(line)


def is_valid_json_line(line: str) -> bool:
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ JSON —Å—Ç—Ä–æ–∫–∏"""
    return FieldExtractor.is_valid_json_line(line)


@contextmanager
def parser_context(config: Optional[ParserConfig] = None):
    """
    –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞—Ä—Å–µ—Ä–æ–º
    
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    ```python
    with parser_context() as parser:
        results = parser.parse_batch(lines)
    ```
    """
    parser = DataParser(config)
    
    try:
        yield parser
    finally:
        # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ
        parser.clear_cache()


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
get_global_parser()